# 1-3_LLM의 구축 단계

## **1) 레이블이 없는 원시 텍스트(수조 개의 단어)** 
- 예시
	- 인터넷 텍스트   
	- 책   
	- 위키백과   
	- 연구 논문    
- 설명
	- LLM을 레이블이 없는 텍스트 데이터에서 사전 훈련한다.

## **2) 사전 훈련된 LLM(파운데이션 모델)**   

## **3) 레이블이 있는 데이터셋**   
- 설명
	- 사전 훈련된 LLM을 레이블이 있는 데이터셋에서 추가로 훈련하여 특정 작업에 맞게 미세 튜닝된 LLM을 얻음  

## **4) 미세 튜닝된 LLM**   
- 예시 
	- 분류   
	- 요약   
	- 번역   
	- 개인 비서   

## **5) 결과**
- 텍스트 완성   
- 퓨-샷 학습
- 사전 훈련 후에 LLM은 몇 가지 기본적인 능력을 가짐


# 1-4_트랜스포머-구조

```
1. 번역할 텍스트 입력
2. 인코더를 위해 입력 텍스트를 전처리
```

```
[인코더]
3. 인코더는 입력 텍스트 전체를 참조하여 디코더에서 사용할 텍스트 임베딩을 생성
4. 인코더는 디코더의 입력으로사용될 임베딩을 반환
```

```
5. 출력 텍스트의 일부 : 모델은 한 번에 한 단어씩 번역을 완성
6. 디코더를 위해 입력 텍스트를 전처리
```

```
[디코더]
7. 디코더는 한 번에 한 단어씩 번역된 텍스트를 생성
```

```
[출력]
8. 완성된 출력(번역)
```

   

